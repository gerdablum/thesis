\chapter{Implementation} \label{implementation}
In this chapter we take a look at the application from the developer's point of view. It is a documentation of the development process of the VR application. At the beginning, we give an introduction into the hardware and software tools on a technical base. Later we describe the development process of the smart city scene and the software engineer scene from the environment design to the deployment to the device. Problems and challenges which occurred during the development will be discussed and their solutions presented.

\section{Overview about Unity}
\textit{Unity} is a multi platform game engine. A game engine holds a bundle of tools which are necessary for game development such as tools for graphical design, audio and scripting. Game engines support developing games or applications with a lot of 2D and 3D graphics and also support developing VR applications. \cite{Dickson.2017}\\
For the development of our game, the engine Unity and the \textit{Oculus Unity SDK} are used. The Oculus Unity SDK is a support library for Samsung Gear VR and Oculus Go. There are other game engines besides Unity, but we chose to work with this engine engine because it is beginner friendly, according to \cite{Dickson.2017}, and free for personal and academic use. Another positive aspect of Unity is the large community from hobby game developers to professional teams. The Unity asset store provides many 3D objects, support libraries, textures, audios and other useful tools for game development. This so called assets can be imported into the engine very easily. There is a variety of free assets as well as paid assets.\\
\begin{figure}[h!]
  \includegraphics[width=16cm]{kapitel/eps/editor.pdf}
  \centering
  \caption{Screenshot of Unity editor}
  \label{fig:unity-editor}
\end{figure}
The Unity editor, as seen in figure \ref{fig:unity-editor}, is the main tool which is used to create our game. It has a variety of views for designing the environment, modifying and testing the game. \\
In the following, some concepts of Unity will be explained, which are  relevant for the implementation process of the prototype \cite{unity.2019}:
\paragraph{Game objects} Game objects in Unity are a very important concept. They can be 3D world objects, lights, cameras and special effects. A game object can also be seen as a container which can contain other game objects. Game objects can be relocated and modified in the Unity editor. They can be visible items in the scene or be invisible and work as a trigger area for example.
\paragraph{Components} A component in Unity defines a specific property and always belongs to a game object. Every game object can have a variety of components attached. Components can change the appearance and behaviour of a game object in the game. There are several build-in components available in Unity, but through the scripting API it is possible to create highly customisable components. Unity gives an overview of all components of the selected game object. New components can be attached in the editor.
\paragraph{Prefabs} Prefabs are reusable game objects with components. Once a game object is copied into the prefabs folder in the project, it can be reused between different scenes and even between different projects. The properties of the component are saved within the prefab.
\paragraph{Materials and shaders} Materials can be seen as the skin of game objects. Materials define how the surface of objects look like. The material is a component which can be modified in the Unity editor. Shaders define what properties of a material can be edited. Shaders are scripts which define the rendering of the material through algorithms. A shader defines which input parameters can be modified in the material component.

\subsection{Scripting in Unity}
Unity provides a variety of predefined component, which can be attached to game objects, such as a Rigidbody for physical behaviour or an audio source for playing music. However, it is possible to create components and attach a script to it for customizing the behaviour of a game object. A script can either be written in C\# or a  modified JavaScript by Unity. When using C\#, the .NET environment is the base for scripting in Unity. For this prototype application, C\# is chosen, because it is the wider used programming language within the Unity community. \\
Every component is a class and inheritates from the \texttt{MonoBehaviour} class. Through the scripting API all existing game objects within the game and their components can be accessed and modified, it is possible to create new game objects, react to events, load new scenes and control the game flow.
\texttt{MonoBehaviour} is a class from the Unity scripting API, which every game object inheritates from. It contains several methods which can be overwritten to execute code after various lifecycle events. The update method for example is called every frame update of the game object. It can be used  to get continues updated values from the game object for example. The start method is called once before the gameplay starts. This method is mostly used for initialization of other game objects or components. \cite{unity.20192}\\
In general, all rules, best practices and design patterns which are common in the .NET environment can be used within Unity as well. Initialization of variables can be done by assigning values in the code or assigning values in the editor with the component view. The second approach is only possible with object variables which are public. This is why many variables are public in this project, although they are not used outside their own class.

\subsection{Unity and Samsung Gear VR support} \label{gearvrsupport}
In Unity, VR support can be simply enabled by ticking a box in the player settings. This invokes rendering the graphics in a split screen mode with one screen for each eye. However, to make a game which is compatible with the Samsung Gear VR or the Oculus Go, more work has to be done. First of all, we have to import the Oculus support library. It can be downloaded from the Unity asset store or the Oculus developer website: \cite{Occulus.2018}. This library contains a variety of components and prefabs. For this prototype the \texttt{OVRCharacterController} prefab from the SDK is used. It contains all necessary scripts and objects for the communication between Unity and the VR hardware. The \texttt{OVRCharacterController} is a first person controller which can be used in the VR environment. It controls the camera from a first person perspective. The support library also contains handheld controller support. There is a prefab which has a script called \texttt{OVRInput} attached. This script can register connected controllers and input from Samsung Gear VR as well as from Oculus Go.\\
To make the game playable on the HMD, the above described prefabs have to be dragged into the scene. Additional to the  texttt{OVRCharacterController}, a handheld controller representation has to be added to the scene. This can be done by attaching a controller prefab to the player game object. This prefab displays a controller model in the virtual environment and translates controller rotation into the game.\\ After finishing this tasks, the prototype can be played in VR. However, user interactions like walking, selection and manipulation are not possible yet. How these interactions are implemented in the game are explained in section \label{inputmethods}. For the complete task in this section, there was no need to write code yet. All we did was adding examples and prefabs from the Oculus support library to our scene.
\newpage
\section{Implementation of the smart city scene}
According to the storyboard from chapter \ref{design}, the smart city scene is the first scene of the game and the entry point for the different labs which represent IT job roles. This section describes the implementation of this scene, including the scene design, the gameplay and the deployment to the HMD. It also describes solving the challenges which occurred during the implementation, such as the dialogue system and supporting of different input methods.
\subsection{Scene Design}
Before starting programming of the game logic, the environment of the city scene has to be designed. This is an important step in the implementation because the look of the environment defines a first impression and has impact when it comes to immersion. The more realistic the environment is designed, the more present users feel in the virtual world.\\
Skyboxes are a concept in Unity to design the background of the game. They contain of a picture wrapped around the scene to give an impression of a wider environment. \cite{unity.20193}
\begin{figure}[h!]
  \includegraphics[width=16cm]{kapitel/explore-world.PNG}
  \centering
  \caption{Screenshot of smart city environment}
  \label{fig:game-world-scene}
\end{figure}
For the prototype, an image with an urban environment is chosen. The image is a 360 photography of a place in a city surrounded by skyscrapers. This image gets attached to the skybox component of the main camera. The image is used in all scenes, the world scene as well as the different laboratory scenes, to state out that the user has not left the city. \\
The smart city environment is an urban environment which should look like a South-East-Asian city, because of the location of NYP. The environment should also encourage free exploration. This means that there should be a high level of details and the game objects of the scenes should be of a high quality. To focus the work on the gameplay and on the same time provide a high quality environmental design, the \textit{Urban Environment Pack}\footnote{\url{Link to the asset store: https://assetstore.unity.com/packages/3d/environments/urban/urban-environment-pack-90000}} from the Unity asset store is used. It contains a high variety of pre-designed objects like buildings, streets and traffic lights, plants and urban obstacles. Figure \ref{fig:game-world-scene} shows an extract of the city environment. Initially we used a different asset for the smart city scene. We changed it to the asset seen in \ref{fig:game-world-scene} because we got feedback that the previous environment lacks interesting details and variety. After changing to the new asset the feedback was positive.
This pre designed environment is adapted to fit the requirements of the gameplay. The sample scene of the asset was reduced in size and the buildings were changed to have five outstanding skyscrapers and several less high buildings.
\begin{figure}[]
  \includegraphics[width=16cm]{kapitel/software-building.PNG}
  \centering
  \caption{Software Engineering laboratory entry point.}
  \label{fig:game-world-scene}
\end{figure}
Each skyscrapers is a laboratory where the different job roles can be explained. An example can be seen in figure \ref{fig:game-world-scene}. This buildings will become interactive in the further development process and should be recognized easily.
\newpage
\subsection{Implementation of the dialogue system}
\begin{figure}[h!]
  \includegraphics[width=16cm]{kapitel/eps/uml-dialog.pdf}
  \centering
  \caption{UML class diagram for the dialogue component}
  \label{fig:uml-controller}
\end{figure}
The application contains several conversations with the virtual assistant, as seen in in the storyboard in chapter \ref{design}. The dialogues in the game between the virtual assistant and the user take place in various places and after different events. The implementation of the dialogue component contains setting the input texts, animating the text appearance during the dialogue, controlling user input and displaying the right dialogue texts at the right time. Especially the last task is a challenge, looking at the fact that by the design of the Unity framework, the game runs in a stateless game loop.\\
 The dialogue component is implemented as a reusable component. It should be possible to add new sentences or change the appearance of the sentences without significant expense. Also the same dialogue class should be used within different scenes. Therefore a reusable dialogue component was designed. The architecture of this component and the dependent classes can be seen in figure \ref{fig:uml-controller}. In the following, it is described how the component works.\\
The dialogue component is a combination of the dialogue script and the dialogue panel, a game object which contains the text and the user input buttons. The dialogue script is attached to an empty game object named \texttt{DialogManager}. From there, references to the dialogue panel are set. The sentences, which should be displayed by the dialogue system are represented as a string field, called \texttt{sentences} in the dialogue script. Every entry in \texttt{sentences} is the text which should be displayed on the panel. When the user clicks the continue button, the next entry from \texttt{sentences} is displayed. This variable has a public accessory. Therefore the content of the dialogue can be changed by any class at any time. \\
The dialogue script has a public method called \texttt{StartTyping()} which invokes displaying of the sentences in the string field with a typewriting animation. It also handles the continue button click by the user. Once every sentence is displayed, the dialogue panel disappears automatically. In the scene controller class, it is only necessary to set the content of the sentences and to invoke the \texttt{StartTyping()} method.\\
To display questions and react to answers of the users, it is possible to customize the buttons appearing in the dialogue panel in the game. This is necessary in the world scene when asking the user to enter a building. The user can either agree and invoke a scene change or refuse and continue exploring. An example for this type of dialogue is seen in figure \ref{fig:question-dialog}.
\begin{figure}[h]
  \includegraphics[width=14cm]{kapitel/software-dialog-question.PNG}
  \centering
  \caption{Example of a dialogue, asking to enter a building.}
  \label{fig:question-dialog}
\end{figure}
\\Internally, every button click invokes a callback which is an interface method. This interface can be implemented individually by the controller classes. In the world scene, the "ok" button callback is implemented and loads a new scene. \\The appearance of the dialogue panel with the right sentences at the right time can also be managed in the controller classes. In the world scene, the welcome dialogue should be displayed at the beginning of the game, while the explaining dialogues of the different job role are displayed when clicked on the corresponding building. Managing the correct order is done by implementing a state machine pattern. The \texttt{DialogManager} class holds a reference of a \texttt{stateMachine} variable which defines different states and sets their logical order. The reference is set inside the controller classes. A state machine is an object which implements the \texttt{IStateMachine} interface. The method \texttt{NextState} performs a step through the state machine. The variable \texttt{current} returns the current state, the state machine is at the moment. Every state describes an event or a position in the game. For the world scene, a state machine diagram is displayed in figure \ref{fig:state-machine}: After displaying the beginning dialogue in \texttt{StartDialog}, \texttt{NoDialog} is entered. When the user interacts with a building the software engineer dialogue (or another job role dialogue, implied by \texttt{OtherJobroleDialog}) is displayed, following by the question to enter the lab in \texttt{SEQuestion}. If the user refuses to enter the lab and answers with ``no'' we go back to \texttt{NoDialog}, otherwise there is a scene transition and the current scene ends.\\
\begin{figure}[h]
  \includegraphics[width=14cm]{kapitel/dialog-state-diagram.png}
  \centering
  \caption{State machine diagram of the dialog flow in the world scene.}
  \label{fig:state-machine}
\end{figure}

Because the \texttt{DialogManager} is a reusable component, it should not know in which state the game is currently. The task of the \texttt{DialogManager} class is only to display all referred sentences when the method is invoked. After all sentences are displayed, the \texttt{NextState()} method is called, to let the controller classes know, that the displaying of the dialogue is finished. 

\newpage
\subsection{Deploying to VR headset}
An important process in the development flow of the VR prototype is testing. Besides from testing the game in the Unity editor, it should be tested on the device itself. This is necessary, because the look and feel, the input methods and the distances differ from playing the game on desktop and playing it in VR.\\
We had two different devices available for testing: Samsung Gear VR and the Oculus Go. They are described in more detail in section \ref{hardware}. At the beginning of the development process only Samsung Gear VR was available. To deploy the game to this device, a compatible mobile phone has to be used. In this project, we used Samsung Note5. The first step is to switch on the developer settings on the phone and allow USB debugging. When using the device for testing the first time, the game has to be registered at Oculus developer dashboard. This is a required step when using the Oculus Integration libraries and is done by downloading a file containing a unique key. This file has to be stored in the \\texttt{Plugins} folder of the project.
After this step, set the project in Unity to Android as a target platform. Unity is a multi platform game engine, and therefore it is not necessary to change the code when switching a platform, only a change in the settings is necessary.\\
The next step is to build the game and transfer it to the phone. For this task, the phone has to be connected via USB to the computer. The building process is invoked through Unity. The game engine builds an executable \texttt{apk} file and transfers it to the phone with the help of the Android Debug Bridge (adb). To finally test the application, the \texttt{apk} file is opened on the phone and the phone has to be attached to the HMD.\\
Later during the development process, Oculus Go headset was provided by the school to test the application. Since this device runs on Android OS, the deploying process is very similar to Samsung Gear VR. Both devices use the Oculus Integration library. Before deploying, developer settings have to be enabled on Oculus Go. This can be done by a configuration app on any phone connected to the HMD. The device also has to be registered to the Oculus Developer Dashboard. For building the game, Oculus Go has to be connected with the PC via USB. We deployed the game on the device directly and no phone has to be attached to the HMD. The process of building with Unity works the same way as with Samsung Gear VR.\\
Comparing both deploying processes, the deployment to Oculus Go was preferred. When testing with Samsung Gear VR the phone had to be detached from the HMD and connected to the computer every time. After the game was built, the phone had to be attached again. This process of attaching and detaching was time consuming, compared to Oculus Go, where simply a wired connection had to be established for building.
\subsection{Support of different input methods}
\label{inputmethods}
Deploying the application to the VR headset takes a few minutes every time. During the development, a continuous testing is necessary. We tested the game every time when completing a task on the board. During testing, we played played the part of the game we changed and tried to cover as much scenarious as possible. Sometimes, VR specific functionalities, such as walking with the controller, had to be tested, but most of the time the testing affected only the gameplay changes. If the testing would always be done on the VR device, it would be too time consuming. Therefore the game is only deployed on the VR headset when it is necessary. In other cases the game is played on the desktop. \\
The problem of this procedure is that the input methods of both target platform differ: For the VR headset, the handeld controller is used, while on the desktop PC mouse and keyboard are the primary input methods. Therefore a solution has to be found on how to make the game playable on both platforms without changing the code when switching the target platforms.\\ 
To remember, Samsung Gear VR as well as Oculus Go use a handheld controller as user input method. Figure \ref{fig:controller} displays the input buttons of the Samsung Gear VR controller in more detail. On the other hand, the keyboard and mouse is used as input device when the game is played on desktop.\\
\begin{figure}[h!]
  \includegraphics[width=10cm]{kapitel/eps/samsung-controller.pdf}
  \centering
  \caption{Handheld controller with button description. Edited from \cite{Samsung.2019b}}
  \label{fig:controller}
\end{figure}
When switching from the desktop platform to Android platform with the handheld controller, following input methods need to be implemented in a different way:
\begin{itemize}
\item Looking around
\item Relocating
\item Selection of objects
\end{itemize}
Looking around in the game is done by a component named \texttt{MouseLook} in the desktop version of the game. This component is a script from the Unity standard assets which is a free utilities library available in the asset store. It manages looking around in a first person view in the direction of the mouse position. When switching the target platform to mobile VR, no mouse position is available, therefore the script does not work. Instead of the \textt{MouseLook} script, the \texttt{OVRPlayerController}, which was described in chapter \ref{gearvrsupport} is attached to the first person controller. It manages looking around in the direction of the head movement instead of the mouse movement.\\
Relocating on the desktop target platform is done with the WASD keys of the keyboard, an input method used in many computer games. The movement is also implemented by a script from the Unity standard assets. However, in a virtual environment it is not possible to use the keyboard. Instead, the relocation is done by pressing the trigger button of the controller to move forward with a constant speed. The direction of moving is set by the direction in which the user is currently looking.\\
\begin{figure}[h!]
  \includegraphics[width=13cm]{kapitel/eps/uml-input.pdf}
  \centering
  \caption{UML class diagram for managing different target platforms}
  \label{fig:uml-universalinput}
\end{figure}
Selection of objects is done with the ray-casting method in both target platforms. How ray-casting in Unity works, is seen in example \ref{raycastcode}. The ray is created from the mouse position in line 2, which is always the center of the screen. In line 3 it is checked whether the ray hit a game object. If the condition is true, the game object which was pointed at is returned in the \texttt{hit} variable.
\begin{lstlisting} 
RaycastHit hit;
Ray ray = camera.ScreenPointToRay(Input.mousePosition);
if (Physics.Raycast(ray, out hit))
{
    Transform objectHit = hit.transform;
}
\end{lstlisting}
\label{raycastcode}
\captionof{lstlisting}{simple ray-casting }
First of all a ray is created from a fixed starting point. If the ray hit an object, this object is stored as a Hit in the variable hit. The difference of ray-casting on the desktop and the VR version is that the ray comes from the center of the main camera in the desktop version, whereas in the VR version the ray is initialized from the controller in the game. In order to make the game work on the VR device, the initialization of the ray has to be changed.
\begin{lstlisting} 
RaycastHit hit;
if (Physics.Raycast(controller.transform.position,
    controller.transform.forward, out hit))
{
    return hit.collider.gameObject;
}
\end{lstlisting}
\label{raycastcode-new}
\captionof{lstlisting}{Ray initialized from the controller representation in the game}

The method \texttt{Physics.Raycast} in line 2 creates the ray from the controller's position instead of the center screen this time. The method checks for objects hit by the ray at the same time.\\
When changing every ray creation which is done in the game's code to the above showed solution, the game is working on the mobile VR platform but it is not working on the desktop anymore. In order to use ray-casting on every target platform without changing the code every time, an additional class has to be added. This class is able to detect which is the current target platform and also if the game is running in the editor gameplay mode. The class creates the ray from either the main camera or the current connected controller depending on the current target platform. It is also able to detect, if a controller is connected and if the controller is set as left or right hand used. In the UML-diagram in figure \ref{fig:uml-universalinput}, the UniversalInput class fulfills the above explained functionalities. The UML-diagram shown below only shows the method and fields relevant for the problem case of this chapter. Subclasses which are used by the UniversalInput class are not displayed in the diagram. The controller classes of the different game scenes use the methods of the UniversalInput class to get the selected object from the ray. It is now possible to play the application with the Samsung Gear VR and use the controller to point at selectable objects. At the same time, the game can be played on a desktop with selecting objects through the main camera focus.

\section{Implementation of the software engineer scene}
The software engineer scene is a laboratory which can be entered through the world scene. The scene contains a mini game through which users can experience programming themselves. The laboratory design is held in a futuristic look with a lot of glass materials and a high amount of white and grey colors. Robots and drones as decoration element and futuristic furniture assets underline the futuristic look of the laboratory. One of the main challenges of this scene was to develop the coding mini game. The objective of the game is to write the code for a drone, which should be able to pickup a package. Another challenge, which occurred in all scenes of the game, but particularly in the software engineer scene was the user guidance. This section describes which task were done in this scene.
\subsection{Coding mini game}
As defined in the storyboard, the mini game in the software engineer scene is about arranging five different code blocks in a logical order. The code blocks contain method calls and a "if" control sequence. In order to start the mini game, the user has to sit on the work desk in the scene. \\
The screenshot in figure \ref{TODO} describes the view of the mini game after sitting down on the chair. The code plates seen in the picture can be selected and dragged into the laptop screen. There they appear as code text on the screen. The buttons above the laptop are to exit the game, return the last step, enable or disable user input hints and to compile the game. \\
Every code plate in the minigame has a component called \texttt{Draggable} attached to the game object. Through this component, objects follow the main camera direction or the controller direction when clicked on it. For releasing objects, users have to click again. Every time, a code plate collides with the screen object of the laptop, the plate disappears and the text on the plates appears on the screen. This is managed through the \texttt{Programming} class which is responsible for all visible changes in the mini game. Every time users drag a code plate into the screen, the name of the game object is added to the static string list variable \texttt{currentOrder}. This variable is stored inside the static class \texttt{ProgrammingLogic}. The \texttt{ProgrammingLogic} class also contains a list called \texttt{correctOrder}, which represents the right order of the code plate arrangement and a method \texttt{CheckSolution()}, which checks the both lists for equality.\\
If the user solves the mini game correctly, the controller class of the scene invokes an animation of a drone flying around in the room and the user is asked to exit the laboratory.
\subsection{User guidance}
TODO: Maybe describe hint label also
As defined in the requirement in chapter \ref{design}, the game should encourage free exploration. On the same time, the user should always know where to go next and be able to detect interactive objects. In the software engineer scene, the desk, which the user has to click on to start the mini game, changes its texture when pointed a it.
Every game object which should appear as interactive, needs to have a  tag "Clickable", which can be assigned to the editor. Inside the update method of the controller class, it is checked, if the user points at an object with this tag. Then the shader of this object is changed to a shader called \texttt{outlined}, which makes the object appear like in figure \ref{??}. In example \ref{code-highlight}, the current highlighted object is called \texttt{hit}. A reference of this object is stored to the object variable \texttt{focusedObject} in order to remove the shader once the focus changes.
\begin{lstlisting} 
if (hit.tag == "Clickable" || hit.tag == "Draggable")
        {
            var renderer = hit.GetComponent<Renderer>();
            if (renderer != null)
               hit.GetComponent<Renderer>().material.shader = outlineShader;
                focusedObject = hit;
            }
        }
\end{lstlisting} 
\label{code-highlight}
When the focus changes to another object, the standard shader is applied to the previous highlighted object.\\
The above code example can be added to other controller classes as well, to highlight interactive objects in other lab scenes. In the world scene a similar method is used for indicating the builings which can be entered. Instead changing the shaders, the color was changed and a blinking animation was added, to attract the user's attention even more.